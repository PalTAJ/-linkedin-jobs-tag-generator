{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477b88d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T08:39:05.776662Z",
     "start_time": "2021-10-24T08:39:05.771194Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, nltk, spacy, gensim, os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c0a4e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T08:39:06.151602Z",
     "start_time": "2021-10-24T08:39:06.145081Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_directories(path):\n",
    "    \"\"\"list files and directories in a given path\"\"\"\n",
    "    arr = os.listdir(path)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0274cc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:39:24.598711Z",
     "start_time": "2021-10-24T09:39:24.279194Z"
    }
   },
   "outputs": [],
   "source": [
    "top_tags = []\n",
    "\n",
    "def list_directories(path):\n",
    "    \"\"\"list files and directories in a given path\"\"\"\n",
    "    arr = os.listdir(path)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' Lowering text and removing undesirable marks\n",
    "    '''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "    \n",
    "\n",
    "token = ToktokTokenizer()\n",
    "punct = punctuation\n",
    "    \n",
    "def strip_list_noempty(mylist):\n",
    "    \n",
    "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
    "    return [item for item in newlist if item != '']\n",
    "    \n",
    "    \n",
    "def clean_punct(text): \n",
    "    ''' Remove punctuations'''\n",
    "    \n",
    "    words = token.tokenize(text)\n",
    "    punctuation_filtered = []\n",
    "    regex = re.compile('[%s]' % re.escape(punct))\n",
    "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in top_tags:\n",
    "            punctuation_filtered.append(w)\n",
    "        else:\n",
    "            w = re.sub('^[0-9]*', \" \", w)\n",
    "            punctuation_filtered.append(regex.sub('', w))\n",
    "  \n",
    "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
    "        \n",
    "    return ' '.join(map(str, filtered_list))\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def stopWordsRemove(text):\n",
    "    ''' Removing all the english stop words from a corpus\n",
    "    Parameter:\n",
    "    text: corpus to remove stop words from it\n",
    "    '''\n",
    "\n",
    "    words = token.tokenize(text)\n",
    "    filtered = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(map(str, filtered))\n",
    "    \n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags, stop_words=stop_words):\n",
    "    ''' It keeps the lemma of the words (lemma is the uninflected form of a word),\n",
    "    and deletes the underired POS tags\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    texts (list): text to lemmatize\n",
    "    allowed_postags (list): list of allowed postags, like NOUN, ADL, VERB, ADV\n",
    "    '''\n",
    "    lemma = wordnet.WordNetLemmatizer()       \n",
    "    doc = nlp(texts) \n",
    "    texts_out = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        if str(token) in top_tags:\n",
    "            texts_out.append(str(token))\n",
    "            \n",
    "        elif token.pos_ in allowed_postags:\n",
    "            \n",
    "            if token.lemma_ not in ['-PRON-']:\n",
    "                texts_out.append(token.lemma_)\n",
    "                \n",
    "            else:\n",
    "                texts_out.append('')\n",
    "     \n",
    "    texts_out = ' '.join(texts_out)\n",
    "\n",
    "    return texts_out\n",
    "    \n",
    "    \n",
    "def strip_emoji(text):\n",
    "#     print(emoji.emoji_count(text))\n",
    "    new_text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(df,column='description'):\n",
    "    \n",
    "    df[column] = df[column].apply(lambda x: clean_text(x))\n",
    "    df[column] = df[column].apply(lambda x:  BeautifulSoup(x).get_text())\n",
    "    df[column] = df[column].apply(lambda x: strip_emoji(x))\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: clean_punct(x)) \n",
    "    df[column] = df[column].apply(lambda x: stopWordsRemove(x)) \n",
    "    df[column] = df[column].apply(lambda x: lemmatization(x, ['NOUN', 'ADV']))\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_tags(df,column='description'):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
    "    X = vectorizer.fit_transform(df[column].str.lower())\n",
    "    X = X.T.toarray()    \n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "\n",
    "    return [df,vectorizer]\n",
    "\n",
    "\n",
    "def get_sim(df,query,vectorizer,temp):\n",
    "    data = []\n",
    "    q = query;q = [q]\n",
    "    query_vector = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}    \n",
    "    for i in range(15):\n",
    "        \n",
    "        if  np.linalg.norm(df.loc[:, i]) * np.linalg.norm(query_vector) == 0.0:\n",
    "            pass\n",
    "        else:\n",
    "            sim[i] = np.dot(df.loc[:, i].values, query_vector) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(query_vector)\n",
    "    \n",
    "    sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    for k, v in sim_sorted:\n",
    "        if v > 0.0:\n",
    "            data.append([k,v])\n",
    "#             print(\"sim:\", v)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecd440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:35:48.674996Z",
     "start_time": "2021-10-24T09:35:48.666560Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9a5068e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:39:26.871655Z",
     "start_time": "2021-10-24T09:39:26.867770Z"
    }
   },
   "outputs": [],
   "source": [
    "files = list_directories('data')\n",
    "category = []\n",
    "for i in files:\n",
    "    t = ''.join(i.split('_')[:-2])\n",
    "    category.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "121ae96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:42:01.318838Z",
     "start_time": "2021-10-24T09:41:56.784169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# query:  engineering \n",
      "\n",
      "matched Folder:  Neuroscience_170_records.csv\n",
      "matched Folder:  Computer_Science_71_records.csv\n",
      "matched Folder:  Materials_Engineering_&_Materials_Science_170_records.csv\n",
      "matched Folder:  Mechanical_Engineering_175_records.csv\n",
      "matched Folder:  Computer_Engineering_69_records.csv\n",
      "matched Folder:  Management_Information_Systems_24_records.csv\n"
     ]
    }
   ],
   "source": [
    "query = 'engineering'\n",
    "print('############# query: ',query,'\\n')\n",
    "\n",
    "for cat, file in zip(category,files):\n",
    "    df = pd.read_csv(f\"data/{file}\") \n",
    "    df = preprocess_text(df,column='job-title')\n",
    "    tags_df,vectorizer = generate_tags(df,column='job-title')\n",
    "    \n",
    "    temp = tags_df.copy(); temp.reset_index(inplace=True);temp = temp[['index']]\n",
    "        \n",
    "    result = get_sim(tags_df,query,vectorizer,temp)\n",
    "    if len(result) != 0:\n",
    "        \n",
    "        print('matched Folder: ',file )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bdb53fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:41:56.782866Z",
     "start_time": "2021-10-24T09:41:52.317648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# query:  science \n",
      "\n",
      "matched Folder:  Communication_Disorders_Sciences_67_records.csv\n",
      "matched Folder:  Computer_Science_71_records.csv\n",
      "matched Folder:  Mathematics_173_records.csv\n",
      "matched Folder:  Materials_Engineering_&_Materials_Science_170_records.csv\n",
      "matched Folder:  Computer___Information_Systems_69_records.csv\n"
     ]
    }
   ],
   "source": [
    "query = 'science'\n",
    "print('############# query: ',query,'\\n')\n",
    "\n",
    "for cat, file in zip(category,files):\n",
    "    df = pd.read_csv(f\"data/{file}\") \n",
    "    df = preprocess_text(df,column='job-title')\n",
    "    tags_df,vectorizer = generate_tags(df,column='job-title')\n",
    "    \n",
    "    temp = tags_df.copy(); temp.reset_index(inplace=True);temp = temp[['index']]\n",
    "        \n",
    "    result = get_sim(tags_df,query,vectorizer,temp)\n",
    "    if len(result) != 0:\n",
    "        \n",
    "        print('matched Folder: ',file )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa707bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
